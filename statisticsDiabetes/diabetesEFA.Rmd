---
title: "diabetesEFA"
output:
  pdf_document: default
  html_document: default
editor_options: 
  chunk_output_type: inline
---

```
STAT 360, 4/8/22
Tara Sothy
Katie Silva
Max Brantner
Joe Cyriac
Audrey Jenkins
Jonathan Barnes
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(skimr)
library(stringi)
library(leaps)
library(stringr)
library(broom)
library(car)
library(readr)
library(readxl)
library(lmtest)
library(corrplot)
library(moments)
library(psych)
library(GPArotation)
```

```{r}
diabetesIndicators <- read_excel("diabetesIndicators.xlsx")
```

```{r}
data <- diabetesIndicators %>%
  mutate(diabetesTF = as.numeric(ifelse(Diabetes_012 > 1, 1, 0)))
```

```{r}
dataT <- data
dataT$BMI <- 1/data$BMI
dataT$Income <- data$Income^4
dataT$GenHlth <- sqrt(data$GenHlth)
dataT$Education <- data$Education^4
```


Because we only have one response variable, we cannot have any clusters, so we don't have any loading matrices/ calculations for that side of things. We remove two variable when we create the explanatory-variable-only matrix because we have two response variables (one generated by the other, we are only going to be using diabetesTF)


```{r}
dat <- dataT[,c(2:22)]

R <- cor(dat)
eigenVal <- eigen(R)$values
eigenVec <- eigen(R)$vectors
d <- sum(eigenVal >= 1)
d
```


Using Kaiser’s criteria, we have 7 eigen values above the threshold. In other words, we have 7 factors. We chose Kaiser’s criterion because this provided already a substantial number of factors. We did not pick a lower threshold like Joliffe’s criterion because, according to Joliffe, there would be 15 factors. This is a massive jump especially since there are only 20 variables in the original dataset (for explanatory variables). To further this point, later in this analysis, adding additional factors was not helpful anyways. As a result, we used Kaiser’s criteria.  


```{r}
L <- diag(eigenVal[c(1:d)])
V <- eigenVec[,c(1:d)]

A <- V %*% sqrt(L)
```


```{r}
Z <- scale(dat)
inv <- solve(R)

rotOt <- pca(R, nfactors = d, rotate = "varimax")$loadings[]
rotOb <- pca(R, nfactors = d, rotate = "oblimin")$loadings[]

BOt <- inv %*% rotOt
FOt <- Z %*% BOt

BOb <- inv %*% rotOb
FOb <- Z %*% BOb

cor(FOb)
```


In class, we decided that if the correlation is above 0.3 or less than -0.3, this means there is a significant correlation between factors. After looking through the correlation matrix above of our loading matrix that has been altered using oblique rotation, there are no correlations that are less than -0.3 or greater than 0.3. Although there are correlations that are close to this threshold, none of them pass, so an oblique rotation is not needed.  


```{r}
d2 <- d + 2
rotOt1 <- pca(R, nfactors = d2, rotate = "varimax")$loadings[]
print("Orthogonally rotated loading matrix using 7 factors (as per Kaiser's criterion)")
rotOt
print("Orthogonally rotated loading matrix using 9 factors")
rotOt1
```


After performing an orthogonal rotation to get our optimal rotated loading matrix, we found that we had several variables that were complex. We partially resolved this by adding 2 additional factors. After this, we still had four variables that load onto more than one factor. These four variables are BMI, Smoker, PhysActivity, and Age. We were unable to find an amount of factors (within reason) that could contain more variables without complexity, therefore, we decided to retain the mentioned complex variables as individual measured variables for further analysis.  


```{r}
rotOtTrim <- rotOt1[c(1,2,3,6,7,8,9,10,11,12,13,14,15,16,17,18,20,21),]
rotOtTrim2 <- rotOtTrim^2
var <- 0
vec <- c()
for(i in 1:d2)
{
  vec <- append(vec, ((sum(rotOtTrim2[,i]) + (4/d2)) / 21))
  var <- var + ((sum(rotOtTrim2[,i]) + (4/d2)) / 21)
  
}
varOt <- pca(R, nfactors = d2, rotate = "varimax")$Vaccounted

varOt[2,]
vec

var
```


Because we removed some of our variables from our set of factors, we manually calculated the variance retained. Above you can see the original variance, variance with the 4 removed variables accounted for, and the sum of variance explained with our manual calculation. With our 9 factors and 4 individual variables, we account for ~71.32% of the variance within our original variables.


```{r}
commOt <- pca(R, nfactors = d2, rotate = "varimax")$communality
commOt
```

58.17% of the variance in HighBP is retained with our dimensionality reduction 
53.89% of the variance in HighChol is retained with our dimensionality reduction 
90.96% of the variance in CholCheck is retained with our dimensionality reduction 
65.13% of the variance in BMI is retained with our dimensionality reduction 
67.61% of the variance in Smoker is retained with our dimensionality reduction 
54.31% of the variance in Stroke is retained with our dimensionality reduction 
49.63% of the variance in HeartDiseaseorAttack is retained with our dimensionality reduction 
42.06% of the variance in PhysActivity is retained with our dimensionality reduction 
59.91% of the variance in Fruits is retained with our dimensionality reduction 
61.48% of the variance in Veggies is retained with our dimensionality reduction 
73.94% of the variance in HvyAlcoholConsump is retained with our dimensionality reduction 
59.23% of the variance in AnyHealthCare is retained with our dimensionality reduction  
58.27% of the variance in NoDocbsCost is retained with our dimensionality reduction           
58.58% of the variance in GenHlth is retained with our dimensionality reduction             
53.29% of the variance in MentHlth is retained with our dimensionality reduction            
64.68% of the variance in PhysHlth is retained with our dimensionality reduction  
55.89% of the variance in DiffWalk is retained with our dimensionality reduction 
73.43% of the variance in Sex is retained with our dimensionality reduction 
64.39% of the variance in Age is retained with our dimensionality reduction         
65.17% of the variance in Education is retained with our dimensionality reduction             
64.88% of the variance of Income is retained with our dimensionality reduction 

```{r}

```



